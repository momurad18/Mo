{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "-3YMe-tVR-sf"
      ],
      "authorship_tag": "ABX9TyNVszC8PXPCm+Xi8VBvd9yS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utechs/Mo/blob/main/lda_on_instagram_post_captions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install some required libraries"
      ],
      "metadata": {
        "id": "8J8GukGtPsQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pyLDAvis ##chart library\n",
        "!pip3 install contractions ##This package is capable of resolving contractions\n",
        "!pip3 install clean-text[gpl]\n",
        "!pip3 install google-cloud-vision ## image processing library"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_HC4FguVIr0",
        "outputId": "c182fdee-4a4b-4bbb-cfe4-5d5ef22cff77"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.8/dist-packages (3.3.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (1.3.5)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (2.8.4)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (0.0.post1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (1.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (57.4.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (1.17)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (1.0.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.0->pyLDAvis) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim->pyLDAvis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.8/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.8/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.8/dist-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.8/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: clean-text[gpl] in /usr/local/lib/python3.8/dist-packages (0.6.0)\n",
            "Requirement already satisfied: ftfy<7.0,>=6.0 in /usr/local/lib/python3.8/dist-packages (from clean-text[gpl]) (6.1.1)\n",
            "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from clean-text[gpl]) (1.7.0)\n",
            "Collecting unidecode<2.0.0,>=1.1.1\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy<7.0,>=6.0->clean-text[gpl]) (0.2.5)\n",
            "Installing collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.8/dist-packages (3.1.4)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-vision) (3.19.6)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-vision) (2.8.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-vision) (1.22.1)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (2.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (1.57.0)\n",
            "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (1.48.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (1.51.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (1.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') ## giving colab access to your drive"
      ],
      "metadata": {
        "id": "8ZnKjCa4aNug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Google vision api to get images lables\n",
        "Please note this paid service don't use it frequently"
      ],
      "metadata": {
        "id": "-3YMe-tVR-sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "import os\n",
        "\n",
        "def detect_labels_uri(uri):\n",
        "    \"\"\"Detects labels in the file located in Google Cloud Storage or on the\n",
        "   Web.\"\"\"\n",
        "    from google.cloud import vision\n",
        "    client = vision.ImageAnnotatorClient()\n",
        "    image = vision.Image()\n",
        "    image.source.image_uri = uri\n",
        "\n",
        "    response = client.label_detection(image=image)\n",
        "    labels = response.label_annotations\n",
        "    return labels\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= \"/content/drive/My Drive/key.json\" # you need to generate your own key to use google vision api\n",
        "df = pd.read_excel(\"/content/drive/My Drive/influencers-data.xlsx\") # file contain images urls which collected locally on my pc using some custom script\n",
        "# Start of Function \n",
        "def extract_labels_from_gvision(url):\n",
        "    my_labels = detect_labels_uri(url)\n",
        "    my_label_list = list()\n",
        "    for label in my_labels:\n",
        "        my_label_list.append(label.description)\n",
        "    return my_label_list\n",
        "# End of Function\n",
        "df['labels'] = df['display_url'].apply(extract_labels_from_gvision) # excute the extract_labels_from_gvision function on all image url\n",
        "df.to_excel(\"/content/drive/My Drive/influencers-data-with-images-lables.xlsx\",index=False) # append lables to pur data and save it to new file"
      ],
      "metadata": {
        "id": "ls74EBUtTeTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP FOR IMAGES LABLES\n",
        "\n",
        "***This section still under development***\n",
        "\n",
        "*   Read data data from excel sheet\n",
        "*   apply some nlp function on the images lables\n",
        "*   caculate  engagement score using likes and comments score\n",
        "\n"
      ],
      "metadata": {
        "id": "U5mDroRdS22m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "import re\n",
        "from pprint import pprint\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "\n",
        "df = pd.read_excel(\"/content/drive/My Drive/with-images-lables.xlsx\") # read data with lables from excel file\n",
        "\n",
        "df['labels'] = df['labels'].apply(lambda x: literal_eval(x))\n",
        "df[\"labels\"] = df[\"labels\"].astype(str)\n",
        "df[\"labels\"] = df[\"labels\"].apply(lambda each_post: word_tokenize(re.sub(r'[^\\w\\s]',' ',each_post.lower())))\n",
        "df[\"labels_strings\"] = df['labels'].apply(' '.join)\n",
        "\n",
        "\n",
        "df[\"likes_normalized\"] = df[\"likes\"]/df[\"likes\"].max() \n",
        "df[\"comments_normalized\"] = df[\"comments\"]/df[\"comments\"].max()\n",
        "\n",
        "# Create engagement score we could change this later\n",
        "df[\"engagement_score\"] = 0.4*df[\"likes_normalized\"] + 0.6*df[\"comments_normalized\"]\n",
        "engagement_median = df[\"engagement_score\"].median()\n",
        "df[\"engagement\"] = df[\"engagement_score\"].apply(lambda x: 1 if x > engagement_median else 0)\n",
        "df.head() # used to show results in console\n",
        "df.to_excel(\"/content/drive/My Drive/with-images-lables-cleaned.xlsx\",index=False)\n"
      ],
      "metadata": {
        "id": "HFX2ZqFNWr-W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "b0aaa5ea-810d-4654-ec21-a55af17c55bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4ea788efc1f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/with-images-lables.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# read data with lables from excel file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1189\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1192\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1071\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/with-images-lables.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import DataFrame, Series\n",
        "import urllib.request \n",
        "import statsmodels.api as sm\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_excel(\"/content/drive/My Drive/suzannaak1-res1.xlsx\")\n",
        "# Keeping all labels intact and using tf-idf score\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['labels_strings'].values.astype('U').tolist())\n",
        "q = vectorizer.get_feature_names()\n",
        "l = pd.DataFrame(X.toarray())\n",
        "for i in range(len(q)):\n",
        "    l = l.rename(columns={i: q[i]}) \n",
        "\n",
        "l['engagement'] = df['engagement']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(l.iloc[:,:-1], l['engagement'], test_size=0.20, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(random_state=0).fit(X_train,y_train )\n",
        "print(\"Accuracy with Image Labels: \" + str(clf.score(X_test, y_test)))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
        "\n",
        "# Keeping all captions intact and using tf-idf score\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['caption'].values.astype('U').tolist())\n",
        "\n",
        "q = vectorizer.get_feature_names()\n",
        "l = pd.DataFrame(X.toarray())\n",
        "for i in range(len(q)):\n",
        "    l = l.rename(columns={i: q[i]}) \n",
        "\n",
        "l['engagementVal'] = df['engagement']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(l.iloc[:,:-1], l['engagementVal'], test_size=0.20, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(X_train,y_train )\n",
        "print(\"Accuracy with Caption Labels: \" + str(clf.score(X_test, y_test)))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
        "\n",
        "df['caption+labels'] = df['caption'] + \" \" + df['labels_strings']\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['caption+labels'].values.astype('U').tolist())\n",
        "q = vectorizer.get_feature_names()\n",
        "l = pd.DataFrame(X.toarray())\n",
        "for i in range(len(q)):\n",
        "    l = l.rename(columns={i: q[i]}) \n",
        "\n",
        "l['engagementVal'] = df['engagement']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(l.iloc[:,:-1], l['engagementVal'], test_size=0.33, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(X_train,y_train )\n",
        "print(\"Accuracy with both Captions and Image Labels: \" + str(clf.score(X_test, y_test)))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, clf.predict(X_test)))"
      ],
      "metadata": {
        "id": "Qh9Mjc7geX7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0B3dLwlU0Cd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from pandas import DataFrame, Series\n",
        "import urllib.request \n",
        "import statsmodels.api as sm\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "df = pd.read_excel(\"/content/drive/My Drive/suzannaak1-res1.xlsx\")\n",
        "df[\"labels\"] = df[\"labels\"].astype(str)\n",
        "df[\"label_tokens\"] = df[\"labels\"].apply(lambda each_post: word_tokenize(re.sub(r'[^\\w\\s]',' ',each_post.lower())))\n",
        "df[\"label_tokens\"] = df[\"label_tokens\"].apply(lambda list_of_words: [x for x in list_of_words if x not in stop_words])\n",
        "df.head()\n",
        "\n",
        "def bigrams(words, bi_min=15, tri_min=10):\n",
        "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
        "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "    return bigram_mod\n",
        "def get_corpus(df):\n",
        "    \"\"\"\n",
        "    Get Bigram Model, Corpus, id2word mapping\n",
        "    \"\"\"\n",
        "    bigram = bigrams(df.label_tokens)\n",
        "    bigram = [bigram[review] for review in df.label_tokens]\n",
        "    id2word = gensim.corpora.Dictionary(bigram)\n",
        "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
        "    id2word.compactify()\n",
        "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
        "    return corpus, id2word, bigram\n",
        "train_corpus, train_id2word, bigram_train = get_corpus(df)\n",
        "import logging\n",
        "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "import warnings\n",
        "import  multiprocessing\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter('ignore')\n",
        "\n",
        "    lda_train = gensim.models.ldamulticore.LdaMulticore(\n",
        "                            corpus=train_corpus,\n",
        "                            num_topics=4,\n",
        "                            id2word=train_id2word,\n",
        "                            chunksize=100,\n",
        "                            workers=7, # Num. Processing Cores - 1\n",
        "                            passes=80,\n",
        "                            eval_every = 1,\n",
        "                            per_word_topics=True)\n",
        "    lda_train.save('lda_train.model')\n",
        "    coherence_model_lda = CoherenceModel(model=lda_train, texts=bigram_train, dictionary=train_id2word, coherence='c_v')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "    print(\"The coherence of the LDA model is\",coherence_lda)\n",
        "    vis = pyLDAvis.gensim_models.prepare(topic_model=lda_train, corpus=train_corpus, dictionary=train_id2word)\n",
        "    pyLDAvis.save_html(vis, '/content/drive/My Drive/5S2lda.html')\n",
        "    df_lda = pd.DataFrame(lda_train.show_topics(), columns=['Topic','Word Weights'])\n",
        "    df_lda.head()\n",
        "    df_lda.to_excel('/content/drive/My Drive/5S2Word_Weights.xlsx')\n",
        "    lda_train.show_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP FROM POSTS CAPTIONS\n",
        "\n",
        "\n",
        "*   Read data data from excel sheet\n",
        "*   Caculate engagement score using likes and comments\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "*   Apply some nlp function on the images lables\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3cZNFqOUvCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "\n",
        "df = pd.read_excel(\"/content/drive/My Drive/influencers-data.xlsx\") ## all influencers data\n",
        "df[\"likes_normalized\"] = df[\"likes\"]/df[\"likes\"].max() \n",
        "df[\"comments_normalized\"] = df[\"comments\"]/df[\"comments\"].max()\n",
        "## Create engagement score\n",
        "df[\"engagement_score\"] = 0.4*df[\"likes_normalized\"] + 0.6*df[\"comments_normalized\"]\n",
        "engagement_median = df[\"engagement_score\"].median()\n",
        "df[\"engagement\"] = df[\"engagement_score\"].apply(lambda x: 1 if x > engagement_median else 0)\n",
        "df.to_excel(\"/content/drive/My Drive/influencers-data-engagement-score.xlsx\",index=False)"
      ],
      "metadata": {
        "id": "sGF1-M_lhcLW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "from cleantext import clean\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "df = pd.read_excel(\"/content/drive/My Drive/influencers-data-engagement-score.xlsx\")\n",
        "# start hashtags functions\n",
        "def extract_hashtags(text):\n",
        "  textList = text.split()\n",
        "  hashtags = [item for item in textList if item[0] == \"#\"]\n",
        "  return hashtags\n",
        "\n",
        "def remove_hashtags(text):\n",
        "  textList = text.split()\n",
        "  text =  [item for item in textList if item[0] != \"#\"]\n",
        "  return (' ').join(text)\n",
        "\n",
        "# end hashtags functions\n",
        "\n",
        "#df['caption'] = df['caption'].apply(lambda x: literal_eval(x))\n",
        "df[\"caption\"] = df[\"caption\"].astype(str)\n",
        "df[\"hashtags\"] = df[\"caption\"].apply(lambda each_post: extract_hashtags(each_post))\n",
        "df[\"caption_without_hash\"] = df[\"caption\"].apply(lambda each_post: remove_hashtags(each_post))\n",
        "df[\"caption_without_hash\"] = df[\"caption_without_hash\"].apply(lambda each_post: contractions.fix(each_post))\n",
        "df[\"caption_without_hash\"] = df[\"caption_without_hash\"].apply(lambda each_post: clean(each_post, no_emoji=True))\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "doc = df[\"caption_without_hash\"].apply(lambda each_post: nlp(each_post))\n",
        "df[\"caption_lemma\"] = doc.apply(lambda each_post: (\" \".join([token.lemma_ for token in each_post])))\n",
        "\n",
        "df.to_excel(\"/content/drive/My Drive/influencers-data-cleaned.xlsx\",index=False) \n"
      ],
      "metadata": {
        "id": "p9kqEbbk8tK9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import words\n",
        "import re\n",
        "from pprint import pprint\n",
        "# spacy for STOP WORDS\n",
        "sp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "spacy_stopwords = sp.Defaults.stop_words\n",
        "words = set(words.words())\n",
        "spacy_stopwords |= {\"jest\",\"sie\", \"nan\"} # Add to stop words list\n",
        "df = pd.read_excel(\"/content/drive/My Drive/influencers-data-cleaned.xlsx\") # Read file from drive\n",
        "\n",
        "df[\"caption_lemma\"] = df[\"caption_lemma\"].astype(str)\n",
        "df[\"caption_lemma_clean\"] = df[\"caption_lemma\"].apply(lambda each_post: re.sub(r\"[^a-zA-Z0-9 ]\", \"\", each_post))\n",
        "df[\"caption_tokens\"] = df[\"caption_lemma_clean\"].apply(lambda each_post: word_tokenize(re.sub(r'[^\\w\\s]',' ',each_post.lower())))\n",
        "df[\"caption_tokens\"] = df[\"caption_tokens\"].apply(lambda list_of_words: [x for x in list_of_words if x not in spacy_stopwords])\n",
        "df[\"caption_tokens\"] = df[\"caption_tokens\"].apply(lambda list_of_words: [x for x in list_of_words if x in words or not x.isalpha()])\n",
        "df[\"caption_tokens\"] = df[\"caption_tokens\"].apply(lambda list_of_words: [x for x in list_of_words if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())])\n",
        "df[\"caption_tokens\"] = df[\"caption_tokens\"].apply(lambda list_of_words: [x for x in list_of_words if len(x) > 2 ])\n",
        "\n",
        "df.to_excel('/content/drive/My Drive/influencers-data-with-tokens.xlsx') # Save file to drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6aaSrxb5rpx",
        "outputId": "3ce2c52c-8124-4269-d683-1018fe2e0028"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from pandas import DataFrame, Series\n",
        "import urllib.request \n",
        "import statsmodels.api as sm\n",
        "import math\n",
        "\n",
        "df = pd.read_excel(\"/content/drive/My Drive/influencers-data-with-tokens.xlsx\")\n",
        "\n",
        "def bigrams(words, bi_min=15, tri_min=10):\n",
        "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
        "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "    return bigram_mod\n",
        "def get_corpus(df):\n",
        "    \"\"\"\n",
        "    Get Bigram Model, Corpus, id2word mapping\n",
        "    \"\"\"\n",
        "    bigram = bigrams(df.caption_tokens)\n",
        "    bigram = [bigram[review] for review in df.caption_tokens]\n",
        "    id2word = gensim.corpora.Dictionary(bigram)\n",
        "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
        "    id2word.compactify()\n",
        "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
        "    return corpus, id2word, bigram\n",
        "train_corpus, train_id2word, bigram_train = get_corpus(df)\n",
        "import logging\n",
        "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "import warnings\n",
        "import  multiprocessing\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter('ignore')\n",
        "    lda_train = gensim.models.ldamulticore.LdaMulticore(\n",
        "                            corpus=train_corpus,\n",
        "                            num_topics=4,\n",
        "                            id2word=train_id2word,\n",
        "                            chunksize=100,\n",
        "                            workers=7, # Num. Processing Cores - 1\n",
        "                            passes=80,\n",
        "                            eval_every = 1,\n",
        "                            per_word_topics=True)\n",
        "    lda_train.save('lda_train.model')\n",
        "    coherence_model_lda = CoherenceModel(model=lda_train, texts=bigram_train, dictionary=train_id2word, coherence='c_v')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "    print(\"The coherence of the LDA model is\",coherence_lda)\n",
        "    vis = pyLDAvis.gensim_models.prepare(topic_model=lda_train, corpus=train_corpus, dictionary=train_id2word)\n",
        "    pyLDAvis.save_html(vis, '/content/drive/My Drive/chart-lda.html')\n",
        "    df_lda = pd.DataFrame(lda_train.show_topics(), columns=['Topic','Word Weights'])\n",
        "    df_lda.head()\n",
        "    df_lda.to_excel('/content/drive/My Drive/Word_Weights.xlsx')\n",
        "    lda_train.show_topics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIX4_aQfZvLp",
        "outputId": "21ead6f2-f70f-413b-dc23-a3b85393609e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The coherence of the LDA model is 0.642476230043491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_vecs = []\n",
        "for i in range(len(df.label_tokens)):\n",
        "  top_topics = lda_train.get_document_topics(train_corpus[i], minimum_probability=0.0)\n",
        "  topic_vec = [top_topics[i][1] for i in range(4)]\n",
        "  train_vecs.append(topic_vec)\n",
        "train_vec_df=pd.DataFrame(train_vecs)\n",
        "train_vec_df.columns=['topic0','topic1','topic2','topic3']\n",
        "train_vec_df.iloc[31]\n",
        "df_nat_final=pd.concat([df.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)\n",
        "df_nat_final[:2]\n",
        "df_nat_final.to_excel(\"/content/drive/My Drive/STopic_Weights.xlsx\")\n",
        "q1=np.percentile(df_nat_final.engagement_score, 25) \n",
        "q2=np.percentile(df_nat_final.engagement_score, 50)  \n",
        "q3=np.percentile(df_nat_final.engagement_score, 75)\n",
        "\n",
        "print (q1,q2,q3)\n",
        "top_quartile=df_nat_final[df_nat_final['engagement_score']>q3]\n",
        "top_quartile[:3]\n",
        "average_topic_weights_top = top_quartile[[\"topic0\",'topic1',\"topic2\",'topic3']].mean(axis=0)\n",
        "average_topic_weights_top\n",
        "bottom_quartile=df_nat_final[df_nat_final['engagement_score']<q1]\n",
        "bottom_quartile[:3]\n",
        "average_topic_weights_bot = bottom_quartile[[\"topic0\",'topic1',\"topic2\",'topic3']].mean(axis=0)\n",
        "average_topic_weights_bot\n",
        "quartile_topics = pd.concat([average_topic_weights_top,average_topic_weights_bot],axis=1)\n",
        "quartile_topics.columns = ['Top Quartile','Bottom Quartile']\n",
        "quartile_topics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "15pNYf_Ehyv8",
        "outputId": "b5862851-733e-4e6f-eef8-93b7da76c5c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.037846623244339364 0.07428245694367869 0.11016741269589107\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Top Quartile  Bottom Quartile\n",
              "topic0      0.306054         0.279198\n",
              "topic1      0.196484         0.217985\n",
              "topic2      0.162380         0.312911\n",
              "topic3      0.335082         0.189907"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-55cd7a76-ddaa-47d4-babe-7b24b70b474a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Top Quartile</th>\n",
              "      <th>Bottom Quartile</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>topic0</th>\n",
              "      <td>0.306054</td>\n",
              "      <td>0.279198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>topic1</th>\n",
              "      <td>0.196484</td>\n",
              "      <td>0.217985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>topic2</th>\n",
              "      <td>0.162380</td>\n",
              "      <td>0.312911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>topic3</th>\n",
              "      <td>0.335082</td>\n",
              "      <td>0.189907</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55cd7a76-ddaa-47d4-babe-7b24b70b474a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-55cd7a76-ddaa-47d4-babe-7b24b70b474a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-55cd7a76-ddaa-47d4-babe-7b24b70b474a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}
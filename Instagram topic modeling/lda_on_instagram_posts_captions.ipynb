{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNVszC8PXPCm+Xi8VBvd9yS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utechs/Mo/blob/main/Instagram%20topic%20modeling/lda_on_instagram_posts_captions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install some required libraries"
      ],
      "metadata": {
        "id": "8J8GukGtPsQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pyLDAvis ##chart library\n",
        "!pip3 install contractions ##This package is capable of resolving contractions\n",
        "!pip3 install clean-text[gpl]\n",
        "!pip3 install google-cloud-vision ## image processing library"
      ],
      "metadata": {
        "id": "H_HC4FguVIr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') ## giving colab access to your drive"
      ],
      "metadata": {
        "id": "8ZnKjCa4aNug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Google vision api to get images lables\n",
        "Please note this paid service don't use it frequently"
      ],
      "metadata": {
        "id": "-3YMe-tVR-sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "import os\n",
        "\n",
        "def detect_labels_uri(uri):\n",
        "    \"\"\"Detects labels in the file located in Google Cloud Storage or on the\n",
        "   Web.\"\"\"\n",
        "    from google.cloud import vision\n",
        "    client = vision.ImageAnnotatorClient()\n",
        "    image = vision.Image()\n",
        "    image.source.image_uri = uri\n",
        "\n",
        "    response = client.label_detection(image=image)\n",
        "    labels = response.label_annotations\n",
        "    return labels\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= \"/content/drive/My Drive/key.json\" # you need to generate your own key to use google vision api\n",
        "df = pd.read_excel(\"/content/drive/My Drive/influencers-data.xlsx\") # file contain images urls which collected locally on my pc using some custom script\n",
        "# Start of Function \n",
        "def extract_labels_from_gvision(url):\n",
        "    my_labels = detect_labels_uri(url)\n",
        "    my_label_list = list()\n",
        "    for label in my_labels:\n",
        "        my_label_list.append(label.description)\n",
        "    return my_label_list\n",
        "# End of Function\n",
        "df['labels'] = df['display_url'].apply(extract_labels_from_gvision) # excute the extract_labels_from_gvision function on all image url\n",
        "df.to_excel(\"/content/drive/My Drive/influencers-data-with-images-lables.xlsx\",index=False) # append lables to pur data and save it to new file"
      ],
      "metadata": {
        "id": "ls74EBUtTeTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP FOR IMAGES LABLES\n",
        "\n",
        "***This section still under development***\n",
        "\n",
        "*   Read data data from excel sheet\n",
        "*   apply some nlp function on the images lables\n",
        "*   caculate  engagement score using likes and comments score\n",
        "\n"
      ],
      "metadata": {
        "id": "U5mDroRdS22m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "import re\n",
        "from pprint import pprint\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "\n",
        "df = pd.read_excel(\"/content/drive/My Drive/with-images-lables.xlsx\") # read data with lables from excel file\n",
        "\n",
        "df['labels'] = df['labels'].apply(lambda x: literal_eval(x))\n",
        "df[\"labels\"] = df[\"labels\"].astype(str)\n",
        "df[\"labels\"] = df[\"labels\"].apply(lambda each_post: word_tokenize(re.sub(r'[^\\w\\s]',' ',each_post.lower())))\n",
        "df[\"labels_strings\"] = df['labels'].apply(' '.join)\n",
        "\n",
        "\n",
        "df[\"likes_normalized\"] = df[\"likes\"]/df[\"likes\"].max() \n",
        "df[\"comments_normalized\"] = df[\"comments\"]/df[\"comments\"].max()\n",
        "\n",
        "# Create engagement score we could change this later\n",
        "df[\"engagement_score\"] = 0.4*df[\"likes_normalized\"] + 0.6*df[\"comments_normalized\"]\n",
        "engagement_median = df[\"engagement_score\"].median()\n",
        "df[\"engagement\"] = df[\"engagement_score\"].apply(lambda x: 1 if x > engagement_median else 0)\n",
        "df.head() # used to show results in console\n",
        "df.to_excel(\"/content/drive/My Drive/with-images-lables-cleaned.xlsx\",index=False)\n"
      ],
      "metadata": {
        "id": "HFX2ZqFNWr-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import DataFrame, Series\n",
        "import urllib.request \n",
        "import statsmodels.api as sm\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_excel(\"/content/drive/My Drive/suzannaak1-res1.xlsx\")\n",
        "# Keeping all labels intact and using tf-idf score\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['labels_strings'].values.astype('U').tolist())\n",
        "q = vectorizer.get_feature_names()\n",
        "l = pd.DataFrame(X.toarray())\n",
        "for i in range(len(q)):\n",
        "    l = l.rename(columns={i: q[i]}) \n",
        "\n",
        "l['engagement'] = df['engagement']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(l.iloc[:,:-1], l['engagement'], test_size=0.20, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(random_state=0).fit(X_train,y_train )\n",
        "print(\"Accuracy with Image Labels: \" + str(clf.score(X_test, y_test)))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
        "\n",
        "# Keeping all captions intact and using tf-idf score\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['caption'].values.astype('U').tolist())\n",
        "\n",
        "q = vectorizer.get_feature_names()\n",
        "l = pd.DataFrame(X.toarray())\n",
        "for i in range(len(q)):\n",
        "    l = l.rename(columns={i: q[i]}) \n",
        "\n",
        "l['engagementVal'] = df['engagement']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(l.iloc[:,:-1], l['engagementVal'], test_size=0.20, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(X_train,y_train )\n",
        "print(\"Accuracy with Caption Labels: \" + str(clf.score(X_test, y_test)))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
        "\n",
        "df['caption+labels'] = df['caption'] + \" \" + df['labels_strings']\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['caption+labels'].values.astype('U').tolist())\n",
        "q = vectorizer.get_feature_names()\n",
        "l = pd.DataFrame(X.toarray())\n",
        "for i in range(len(q)):\n",
        "    l = l.rename(columns={i: q[i]}) \n",
        "\n",
        "l['engagementVal'] = df['engagement']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(l.iloc[:,:-1], l['engagementVal'], test_size=0.33, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(random_state=0, solver='lbfgs').fit(X_train,y_train )\n",
        "print(\"Accuracy with both Captions and Image Labels: \" + str(clf.score(X_test, y_test)))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, clf.predict(X_test)))"
      ],
      "metadata": {
        "id": "Qh9Mjc7geX7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0B3dLwlU0Cd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from pandas import DataFrame, Series\n",
        "import urllib.request \n",
        "import statsmodels.api as sm\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "df = pd.read_excel(\"/content/drive/My Drive/suzannaak1-res1.xlsx\")\n",
        "df[\"labels\"] = df[\"labels\"].astype(str)\n",
        "df[\"label_tokens\"] = df[\"labels\"].apply(lambda each_post: word_tokenize(re.sub(r'[^\\w\\s]',' ',each_post.lower())))\n",
        "df[\"label_tokens\"] = df[\"label_tokens\"].apply(lambda list_of_words: [x for x in list_of_words if x not in stop_words])\n",
        "df.head()\n",
        "\n",
        "def bigrams(words, bi_min=15, tri_min=10):\n",
        "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
        "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "    return bigram_mod\n",
        "def get_corpus(df):\n",
        "    \"\"\"\n",
        "    Get Bigram Model, Corpus, id2word mapping\n",
        "    \"\"\"\n",
        "    bigram = bigrams(df.label_tokens)\n",
        "    bigram = [bigram[review] for review in df.label_tokens]\n",
        "    id2word = gensim.corpora.Dictionary(bigram)\n",
        "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
        "    id2word.compactify()\n",
        "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
        "    return corpus, id2word, bigram\n",
        "train_corpus, train_id2word, bigram_train = get_corpus(df)\n",
        "import logging\n",
        "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "import warnings\n",
        "import  multiprocessing\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter('ignore')\n",
        "\n",
        "    lda_train = gensim.models.ldamulticore.LdaMulticore(\n",
        "                            corpus=train_corpus,\n",
        "                            num_topics=4,\n",
        "                            id2word=train_id2word,\n",
        "                            chunksize=100,\n",
        "                            workers=7, # Num. Processing Cores - 1\n",
        "                            passes=80,\n",
        "                            eval_every = 1,\n",
        "                            per_word_topics=True)\n",
        "    lda_train.save('lda_train.model')\n",
        "    coherence_model_lda = CoherenceModel(model=lda_train, texts=bigram_train, dictionary=train_id2word, coherence='c_v')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "    print(\"The coherence of the LDA model is\",coherence_lda)\n",
        "    vis = pyLDAvis.gensim_models.prepare(topic_model=lda_train, corpus=train_corpus, dictionary=train_id2word)\n",
        "    pyLDAvis.save_html(vis, '/content/drive/My Drive/5S2lda.html')\n",
        "    df_lda = pd.DataFrame(lda_train.show_topics(), columns=['Topic','Word Weights'])\n",
        "    df_lda.head()\n",
        "    df_lda.to_excel('/content/drive/My Drive/5S2Word_Weights.xlsx')\n",
        "    lda_train.show_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP FROM POSTS CAPTIONS\n",
        "\n",
        "\n",
        "*   Read data data from excel sheet\n",
        "*   Caculate engagement score using likes and comments\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "*   Apply some nlp function on the images lables\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3cZNFqOUvCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "\n",
        "df = pd.read_excel(\"/content/drive/My Drive/influencers-data.xlsx\") ## all influencers data\n",
        "df[\"likes_normalized\"] = df[\"likes\"]/df[\"likes\"].max() \n",
        "df[\"comments_normalized\"] = df[\"comments\"]/df[\"comments\"].max()\n",
        "## Create engagement score\n",
        "df[\"engagement_score\"] = 0.4*df[\"likes_normalized\"] + 0.6*df[\"comments_normalized\"]\n",
        "engagement_median = df[\"engagement_score\"].median()\n",
        "df[\"engagement\"] = df[\"engagement_score\"].apply(lambda x: 1 if x > engagement_median else 0)\n",
        "df.to_excel(\"/content/drive/My Drive/influencers-data-engagement-score.xlsx\",index=False)"
      ],
      "metadata": {
        "id": "sGF1-M_lhcLW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "from cleantext import clean\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "df = pd.read_excel(\"/content/drive/My Drive/influencers-data-engagement-score.xlsx\")\n",
        "# start hashtags functions\n",
        "def extract_hashtags(text):\n",
        "  textList = text.split()\n",
        "  hashtags = [item for item in textList if item[0] == \"#\"]\n",
        "  return hashtags\n",
        "\n",
        "def remove_hashtags(text):\n",
        "  textList = text.split()\n",
        "  text =  [item for item in textList if item[0] != \"#\"]\n",
        "  return (' ').join(text)\n",
        "\n",
        "# end hashtags functions\n",
        "\n",
        "#df['caption'] = df['caption'].apply(lambda x: literal_eval(x))\n",
        "df[\"caption\"] = df[\"caption\"].astype(str)\n",
        "df[\"hashtags\"] = df[\"caption\"].apply(lambda each_post: extract_hashtags(each_post))\n",
        "df[\"caption_without_hash\"] = df[\"caption\"].apply(lambda each_post: remove_hashtags(each_post))\n",
        "df[\"caption_without_hash\"] = df[\"caption_without_hash\"].apply(lambda each_post: contractions.fix(each_post))\n",
        "df[\"caption_without_hash\"] = df[\"caption_without_hash\"].apply(lambda each_post: clean(each_post, no_emoji=True))\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "doc = df[\"caption_without_hash\"].apply(lambda each_post: nlp(each_post))\n",
        "df[\"caption_lemma\"] = doc.apply(lambda each_post: (\" \".join([token.lemma_ for token in each_post])))\n",
        "\n",
        "df.to_excel(\"/content/drive/My Drive/influencers-data-cleaned.xlsx\",index=False) \n"
      ],
      "metadata": {
        "id": "p9kqEbbk8tK9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import words\n",
        "import re\n",
        "from pprint import pprint\n",
        "# spacy for STOP WORDS\n",
        "sp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "spacy_stopwords = sp.Defaults.stop_words\n",
        "words = set(words.words())\n",
        "spacy_stopwords |= {\"jest\",\"sie\", \"nan\"} # Add to stop words list\n",
        "df = pd.read_excel(\"/content/drive/My Drive/influencers-data-cleaned.xlsx\") # Read file from drive\n",
        "\n",
        "df[\"caption_lemma\"] = df[\"caption_lemma\"].astype(str)\n",
        "df[\"caption_lemma_clean\"] = df[\"caption_lemma\"].apply(lambda each_post: re.sub(r\"[^a-zA-Z0-9 ]\", \"\", each_post))\n",
        "df[\"caption_tokens\"] = df[\"caption_lemma_clean\"].apply(lambda each_post: word_tokenize(re.sub(r'[^\\w\\s]',' ',each_post.lower())))\n",
        "df[\"caption_tokens\"] = df[\"caption_tokens\"].apply(lambda list_of_words: [x for x in list_of_words if x not in spacy_stopwords])\n",
        "df[\"caption_tokens\"] = df[\"caption_tokens\"].apply(lambda list_of_words: [x for x in list_of_words if x in words or not x.isalpha()])\n",
        "df[\"caption_tokens\"] = df[\"caption_tokens\"].apply(lambda list_of_words: [x for x in list_of_words if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())])\n",
        "df[\"caption_tokens\"] = df[\"caption_tokens\"].apply(lambda list_of_words: [x for x in list_of_words if len(x) > 2 ])\n",
        "\n",
        "df.to_excel('/content/drive/My Drive/influencers-data-with-tokens.xlsx') # Save file to drive"
      ],
      "metadata": {
        "id": "T6aaSrxb5rpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis # don't skip this\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from pandas import DataFrame, Series\n",
        "import urllib.request \n",
        "import statsmodels.api as sm\n",
        "import math\n",
        "\n",
        "df = pd.read_excel(\"/content/drive/My Drive/influencers-data-with-tokens.xlsx\")\n",
        "\n",
        "def bigrams(words, bi_min=15, tri_min=10):\n",
        "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
        "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "    return bigram_mod\n",
        "def get_corpus(df):\n",
        "    \"\"\"\n",
        "    Get Bigram Model, Corpus, id2word mapping\n",
        "    \"\"\"\n",
        "    bigram = bigrams(df.caption_tokens)\n",
        "    bigram = [bigram[review] for review in df.caption_tokens]\n",
        "    id2word = gensim.corpora.Dictionary(bigram)\n",
        "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
        "    id2word.compactify()\n",
        "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
        "    return corpus, id2word, bigram\n",
        "train_corpus, train_id2word, bigram_train = get_corpus(df)\n",
        "import logging\n",
        "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "import warnings\n",
        "import  multiprocessing\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter('ignore')\n",
        "    lda_train = gensim.models.ldamulticore.LdaMulticore(\n",
        "                            corpus=train_corpus,\n",
        "                            num_topics=4,\n",
        "                            id2word=train_id2word,\n",
        "                            chunksize=100,\n",
        "                            workers=7, # Num. Processing Cores - 1\n",
        "                            passes=80,\n",
        "                            eval_every = 1,\n",
        "                            per_word_topics=True)\n",
        "    lda_train.save('lda_train.model')\n",
        "    coherence_model_lda = CoherenceModel(model=lda_train, texts=bigram_train, dictionary=train_id2word, coherence='c_v')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "    print(\"The coherence of the LDA model is\",coherence_lda)\n",
        "    vis = pyLDAvis.gensim_models.prepare(topic_model=lda_train, corpus=train_corpus, dictionary=train_id2word)\n",
        "    pyLDAvis.save_html(vis, '/content/drive/My Drive/chart-lda.html')\n",
        "    df_lda = pd.DataFrame(lda_train.show_topics(), columns=['Topic','Word Weights'])\n",
        "    df_lda.head()\n",
        "    df_lda.to_excel('/content/drive/My Drive/Word_Weights.xlsx')\n",
        "    lda_train.show_topics()"
      ],
      "metadata": {
        "id": "qIX4_aQfZvLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_vecs = []\n",
        "for i in range(len(df.label_tokens)):\n",
        "  top_topics = lda_train.get_document_topics(train_corpus[i], minimum_probability=0.0)\n",
        "  topic_vec = [top_topics[i][1] for i in range(4)]\n",
        "  train_vecs.append(topic_vec)\n",
        "train_vec_df=pd.DataFrame(train_vecs)\n",
        "train_vec_df.columns=['topic0','topic1','topic2','topic3']\n",
        "train_vec_df.iloc[31]\n",
        "df_nat_final=pd.concat([df.reset_index(drop=True), train_vec_df.reset_index(drop=True)], axis=1)\n",
        "df_nat_final[:2]\n",
        "df_nat_final.to_excel(\"/content/drive/My Drive/STopic_Weights.xlsx\")\n",
        "q1=np.percentile(df_nat_final.engagement_score, 25) \n",
        "q2=np.percentile(df_nat_final.engagement_score, 50)  \n",
        "q3=np.percentile(df_nat_final.engagement_score, 75)\n",
        "\n",
        "print (q1,q2,q3)\n",
        "top_quartile=df_nat_final[df_nat_final['engagement_score']>q3]\n",
        "top_quartile[:3]\n",
        "average_topic_weights_top = top_quartile[[\"topic0\",'topic1',\"topic2\",'topic3']].mean(axis=0)\n",
        "average_topic_weights_top\n",
        "bottom_quartile=df_nat_final[df_nat_final['engagement_score']<q1]\n",
        "bottom_quartile[:3]\n",
        "average_topic_weights_bot = bottom_quartile[[\"topic0\",'topic1',\"topic2\",'topic3']].mean(axis=0)\n",
        "average_topic_weights_bot\n",
        "quartile_topics = pd.concat([average_topic_weights_top,average_topic_weights_bot],axis=1)\n",
        "quartile_topics.columns = ['Top Quartile','Bottom Quartile']\n",
        "quartile_topics"
      ],
      "metadata": {
        "id": "15pNYf_Ehyv8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}